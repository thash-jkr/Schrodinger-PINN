{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS backend for Apple GPU acceleration!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "\n",
    "# Device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS backend for Apple GPU acceleration!\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"MPS not available. Using CPU instead.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselinePINN(nn.Module):\n",
    "    def __init__(self, hidden_layers=6, hidden_units=512):\n",
    "        super(BaselinePINN, self).__init__()\n",
    "        # First layer: input dimension = 2 (x, t)\n",
    "        layers = []\n",
    "        in_dim = 2\n",
    "        \n",
    "        # Build hidden layers\n",
    "        for _ in range(hidden_layers):\n",
    "            layers.append(nn.Linear(in_dim, hidden_units))\n",
    "            in_dim = hidden_units\n",
    "        \n",
    "        # Final output layer: 2 outputs => (psi_real, psi_imag)\n",
    "        out_dim = 2\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.output_layer = nn.Linear(in_dim, out_dim)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        x, t: Tensors of shape [batch_size] each,\n",
    "              representing the coordinates.\n",
    "        Returns (psi_real, psi_imag) each [batch_size].\n",
    "        \"\"\"\n",
    "        # Combine x,t into [batch_size, 2]\n",
    "        x, t = inputs\n",
    "        X = torch.stack((x, t), dim=1)\n",
    "        \n",
    "        # Pass through hidden layers with tanh activation\n",
    "        for layer in self.layers:\n",
    "            X = layer(X)\n",
    "            X = torch.tanh(X)\n",
    "        \n",
    "        # Final linear output\n",
    "        out = self.output_layer(X)\n",
    "        psi_real = out[:, 0]\n",
    "        psi_imag = out[:, 1]\n",
    "        return psi_real, psi_imag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem bounds\n",
    "x_min, x_max = -np.pi, np.pi\n",
    "t_min, t_max = 0.0, 2.0 * np.pi\n",
    "\n",
    "n_collocation = 3140\n",
    "n_boundary = 200\n",
    "n_initial = 314\n",
    "\n",
    "def sample_points():\n",
    "    # Collocation (interior)\n",
    "    x_f = np.random.uniform(x_min, x_max, n_collocation)\n",
    "    t_f = np.random.uniform(t_min, t_max, n_collocation)\n",
    "    \n",
    "    # Boundary: x=±π, random t\n",
    "    t_b = np.random.uniform(t_min, t_max, n_boundary // 2)\n",
    "    x_b_left  = np.full(n_boundary // 2, x_min)\n",
    "    x_b_right = np.full(n_boundary // 2, x_max)\n",
    "    x_b = np.concatenate([x_b_left, x_b_right], axis=0)\n",
    "    t_b = np.concatenate([t_b, t_b], axis=0)\n",
    "    \n",
    "    # Initial: t=0, random x in [-π, π]\n",
    "    x_i = np.random.uniform(x_min, x_max, n_initial)\n",
    "    t_i = np.zeros(n_initial)\n",
    "\n",
    "    # Convert to PyTorch\n",
    "    x_f = torch.from_numpy(x_f).float().to(device)\n",
    "    t_f = torch.from_numpy(t_f).float().to(device)\n",
    "\n",
    "    x_b = torch.from_numpy(x_b).float().to(device)\n",
    "    t_b = torch.from_numpy(t_b).float().to(device)\n",
    "\n",
    "    x_i = torch.from_numpy(x_i).float().to(device)\n",
    "    t_i = torch.from_numpy(t_i).float().to(device)\n",
    "\n",
    "    return x_f, t_f, x_b, t_b, x_i, t_i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(model,\n",
    "                  x_colloc, t_colloc,\n",
    "                  x_bound, t_bound,\n",
    "                  x_init, t_init):\n",
    "    \"\"\"\n",
    "    Computes three losses for a 1D harmonic oscillator Schrödinger PINN in real-imag form:\n",
    "      1) PDE loss: \n",
    "         du/dt + 0.5 d²v/dx² - 0.5 x² v = 0\n",
    "         -dv/dt + 0.5 d²u/dx² - 0.5 x² u = 0\n",
    "      2) Initial condition loss (u(x,0)=ψ0(x), v(x,0)=0)\n",
    "      3) Boundary condition loss (u(±π,t)=0, v(±π,t)=0)\n",
    "\n",
    "    model((x, t)) -> (u, v) = (real part, imaginary part).\n",
    "    \"\"\"\n",
    "\n",
    "    #\n",
    "    # 1) PDE LOSS\n",
    "    #\n",
    "    # Make sure we can take derivatives wrt x_colloc, t_colloc\n",
    "    x_colloc_ = x_colloc.clone().requires_grad_(True)\n",
    "    t_colloc_ = t_colloc.clone().requires_grad_(True)\n",
    "    \n",
    "    # Evaluate model => u, v\n",
    "    u, v = model((x_colloc_, t_colloc_))  # each shape [n_colloc]\n",
    "\n",
    "    # PDE needs partial derivatives\n",
    "    # du/dt, dv/dt, d²u/dx², d²v/dx²\n",
    "    du_dt = torch.autograd.grad(\n",
    "        u, t_colloc_,\n",
    "        grad_outputs=torch.ones_like(u),\n",
    "        create_graph=True\n",
    "    )[0]\n",
    "    dv_dt = torch.autograd.grad(\n",
    "        v, t_colloc_,\n",
    "        grad_outputs=torch.ones_like(v),\n",
    "        create_graph=True\n",
    "    )[0]\n",
    "\n",
    "    du_dx = torch.autograd.grad(\n",
    "        u, x_colloc_,\n",
    "        grad_outputs=torch.ones_like(u),\n",
    "        create_graph=True\n",
    "    )[0]\n",
    "    dv_dx = torch.autograd.grad(\n",
    "        v, x_colloc_,\n",
    "        grad_outputs=torch.ones_like(v),\n",
    "        create_graph=True\n",
    "    )[0]\n",
    "\n",
    "    d2u_dx2 = torch.autograd.grad(\n",
    "        du_dx, x_colloc_,\n",
    "        grad_outputs=torch.ones_like(du_dx),\n",
    "        create_graph=True\n",
    "    )[0]\n",
    "    d2v_dx2 = torch.autograd.grad(\n",
    "        dv_dx, x_colloc_,\n",
    "        grad_outputs=torch.ones_like(dv_dx),\n",
    "        create_graph=True\n",
    "    )[0]\n",
    "\n",
    "    # PDE #1: du/dt + 0.5 d²v/dx² - 0.5 x² v = 0\n",
    "    # PDE #2: -dv/dt + 0.5 d²u/dx² - 0.5 x² u = 0\n",
    "    # We'll form residual1, residual2, then sum squares\n",
    "    term1 = du_dt + 0.5 * d2v_dx2 - 0.5*(x_colloc_**2)*v\n",
    "    term2 = - dv_dt + 0.5 * d2u_dx2 - 0.5*(x_colloc_**2)*u\n",
    "\n",
    "    pde_loss = torch.mean(term1**2 + term2**2)\n",
    "\n",
    "\n",
    "    #\n",
    "    # 2) INITIAL CONDITION LOSS\n",
    "    #\n",
    "    # Evaluate model at t=0\n",
    "    u_i, v_i = model((x_init, t_init))\n",
    "\n",
    "    # The exact initial wavefunction is purely real => v=0\n",
    "    # phi0(x) = π^{-1/4} exp(-x²/2)\n",
    "    # phi1(x) = sqrt(2) x π^{-1/4} exp(-x²/2)\n",
    "    # => superposition (φ0 + φ1)/√2\n",
    "    phi0 = (1/(torch.tensor(torch.pi)**0.25)) * torch.exp(-0.5*(x_init**2))\n",
    "    phi1 = phi0 * (torch.sqrt(torch.tensor(2.0))*x_init)\n",
    "    psi_init_exact = (phi0 + phi1)/torch.sqrt(torch.tensor(2.0))\n",
    "\n",
    "    # We want u(x,0) = psi_init_exact, v(x,0)=0\n",
    "    init_loss = torch.mean((u_i - psi_init_exact)**2) \\\n",
    "              + torch.mean((v_i - 0.0)**2)\n",
    "\n",
    "\n",
    "    #\n",
    "    # 3) BOUNDARY CONDITION LOSS\n",
    "    #\n",
    "    # Evaluate model at x=±π, for random t\n",
    "    u_b, v_b = model((x_bound, t_bound))\n",
    "    # Dirichlet BC => u=0, v=0 on boundary\n",
    "    boundary_loss = torch.mean(u_b**2) + torch.mean(v_b**2)\n",
    "\n",
    "\n",
    "    #\n",
    "    # Return them all\n",
    "    #\n",
    "    return pde_loss, init_loss, boundary_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step=2000, LR=9.00e-04, total=3.960e-02, pde=2.687e-02, init=8.713e-03, bc=4.013e-03\n",
      "Step=4000, LR=8.10e-04, total=1.867e-02, pde=1.430e-02, init=2.246e-03, bc=2.127e-03\n",
      "Step=6000, LR=7.29e-04, total=1.287e-02, pde=1.068e-02, init=9.628e-04, bc=1.223e-03\n",
      "Step=8000, LR=6.56e-04, total=4.415e-02, pde=2.247e-02, init=2.099e-02, bc=6.884e-04\n",
      "Step=10000, LR=5.90e-04, total=9.177e-02, pde=6.494e-02, init=1.696e-02, bc=9.873e-03\n",
      "Step=12000, LR=5.31e-04, total=7.633e-02, pde=5.757e-02, init=1.022e-02, bc=8.542e-03\n",
      "Step=14000, LR=4.78e-04, total=7.270e-02, pde=5.273e-02, init=1.103e-02, bc=8.936e-03\n",
      "Step=16000, LR=4.30e-04, total=3.433e-02, pde=2.546e-02, init=5.047e-03, bc=3.824e-03\n",
      "Step=18000, LR=3.87e-04, total=6.406e-02, pde=4.603e-02, init=1.010e-02, bc=7.923e-03\n",
      "Step=20000, LR=3.49e-04, total=4.858e-02, pde=3.416e-02, init=7.519e-03, bc=6.895e-03\n",
      "Step=22000, LR=3.14e-04, total=6.918e-02, pde=5.008e-02, init=1.038e-02, bc=8.718e-03\n",
      "Step=24000, LR=2.82e-04, total=9.466e-02, pde=6.663e-02, init=1.407e-02, bc=1.396e-02\n",
      "Step=26000, LR=2.54e-04, total=1.517e-02, pde=1.135e-02, init=2.233e-03, bc=1.589e-03\n",
      "Step=28000, LR=2.29e-04, total=3.010e-02, pde=2.154e-02, init=4.710e-03, bc=3.854e-03\n",
      "Step=30000, LR=2.06e-04, total=8.760e-03, pde=6.441e-03, init=1.488e-03, bc=8.310e-04\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the network\n",
    "model = BaselinePINN(hidden_layers=6, hidden_units=512).to(device)\n",
    "\n",
    "# Adam optimizer\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=0.001,\n",
    "    betas=(0.09, 0.999)\n",
    ")\n",
    "\n",
    "# Learning rate scheduler: replicate α_t = α0 * 0.9^(t/2000)\n",
    "def lr_lambda(step):\n",
    "    return 0.9 ** (step / 2000.0)\n",
    "\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "\n",
    "# Sample points once (baseline approach). If you want to re-sample each iteration, do inside loop\n",
    "x_f, t_f, x_b, t_b, x_i, t_i = sample_points()\n",
    "\n",
    "# Training loop\n",
    "num_iterations = 30000\n",
    "print_interval = 2000\n",
    "history = []\n",
    "\n",
    "for step in range(1, num_iterations+1):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    physics_loss, init_loss, bc_loss = loss_function(model, x_f, t_f, x_b, t_b, x_i, t_i)\n",
    "    total_loss = physics_loss + init_loss + bc_loss\n",
    "    \n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()  # apply LR decay\n",
    "    \n",
    "    # Logging\n",
    "    if step % print_interval == 0:\n",
    "        lr_current = scheduler.get_last_lr()[0]\n",
    "        print(f\"Step={step}, LR={lr_current:.2e}, \"\n",
    "              f\"total={total_loss.item():.3e}, \"\n",
    "              f\"pde={physics_loss.item():.3e}, \"\n",
    "              f\"init={init_loss.item():.3e}, \"\n",
    "              f\"bc={bc_loss.item():.3e}\")\n",
    "    \n",
    "    history.append({\n",
    "        \"step\": step,\n",
    "        \"lr\": scheduler.get_last_lr()[0],\n",
    "        \"total_loss\": total_loss.item(),\n",
    "        \"physics_loss\": physics_loss.item(),\n",
    "        \"init_loss\": init_loss.item(),\n",
    "        \"bc_loss\": bc_loss.item()\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nx, Nt = 628, 628\n",
    "x_eval = torch.linspace(x_min, x_max, Nx).to(device)\n",
    "t_eval = torch.linspace(t_min, t_max, Nt).to(device)\n",
    "Xg, Tg = torch.meshgrid(x_eval, t_eval, indexing='ij')  # shape [Nx, Nt]\n",
    "\n",
    "# Flatten\n",
    "X_flat = Xg.reshape(-1)\n",
    "T_flat = Tg.reshape(-1)\n",
    "\n",
    "# Evaluate model\n",
    "psi_r_pred, psi_i_pred = model((X_flat, T_flat))\n",
    "psi_r_grid = psi_r_pred.view(Nx, Nt).detach().cpu().numpy()\n",
    "psi_i_grid = psi_i_pred.view(Nx, Nt).detach().cpu().numpy()\n",
    "\n",
    "# Probability density\n",
    "psi_abs2 = psi_r_grid**2 + psi_i_grid**2\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".intern",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
